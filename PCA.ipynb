{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Machine Learning\n",
    "- Unsupervised learning subsumes all kinds of machine learning where there is no\n",
    "known output, no teacher to instruct the learning algorithm. In unsupervised learning, \n",
    "the learning algorithm is just shown the input data, and asked to extract knowledge from this data.\n",
    "\n",
    "\n",
    "### Types of Unsupervised Learning\n",
    "- Dimensionality Reduction \n",
    "- Clustering\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "- This involves creating a new representation from a high-dimensional data by looking for subset of the original features\n",
    "that summarizes all the essential characteristics in the original high-dimensional data. The most common dimensionlaity\n",
    "reduction techniques is the **Principal Component Analysis (PCA)**.\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "- Principal component analysis (PCA) is a method that rotates the dataset in a way\n",
    "such that the rotated features are statistically uncorrelated. This rotation is often followed \n",
    "by selecting only a subset of the new features, according to how important they\n",
    "are for explaining the data.\n",
    "\n",
    "\n",
    "\n",
    "- The algorithm proceeds by first finding the direction of maximum variance, labeled as “Component 1”. This is the direction in the data that contains most of the information, or\n",
    "in other words, the direction along which the features are most correlated with each\n",
    "other.\n",
    "Then, the algorithm finds the direction that contains the most information while\n",
    "being orthogonal (is at a right angle) to the first direction.\n",
    "\n",
    "\n",
    "\n",
    "- The second plot shows the same data, but now rotated so that the first principal component aligns with the x axis, and the second principal component aligns with the y\n",
    "axis. Before the rotation, the mean was subtracted from the data, so that the transformed \n",
    "data is centered around zero. In the rotated representation found by PCA, the\n",
    "two axes are uncorrelated, meaning that the correlation matrix of the data in this representation is zero except for the diagonal.\n",
    "We can use PCA for dimensionality reduction by retaining only some of the principal\n",
    "components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PCA in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import  StandardScaler \n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SystolicBP</th>\n",
       "      <th>DiastolicBP</th>\n",
       "      <th>BS</th>\n",
       "      <th>BodyTemp</th>\n",
       "      <th>HeartRate</th>\n",
       "      <th>RiskLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>15.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>86</td>\n",
       "      <td>high risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>13.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>70</td>\n",
       "      <td>high risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80</td>\n",
       "      <td>high risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>140</td>\n",
       "      <td>85</td>\n",
       "      <td>7.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>70</td>\n",
       "      <td>high risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>120</td>\n",
       "      <td>60</td>\n",
       "      <td>6.1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>76</td>\n",
       "      <td>low risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  SystolicBP  DiastolicBP    BS  BodyTemp  HeartRate  RiskLevel\n",
       "0   25         130           80  15.0      98.0         86  high risk\n",
       "1   35         140           90  13.0      98.0         70  high risk\n",
       "2   29          90           70   8.0     100.0         80  high risk\n",
       "3   30         140           85   7.0      98.0         70  high risk\n",
       "4   35         120           60   6.1      98.0         76   low risk"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before Applying PCA, we must scale the data so that each feature has a unit variance\n",
    "\n",
    "# get the cancer data\n",
    "\n",
    "df = pd.read_csv (\"mhs.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1014, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age            0\n",
       "SystolicBP     0\n",
       "DiastolicBP    0\n",
       "BS             0\n",
       "BodyTemp       0\n",
       "HeartRate      0\n",
       "RiskLevel      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SystolicBP</th>\n",
       "      <th>DiastolicBP</th>\n",
       "      <th>BS</th>\n",
       "      <th>BodyTemp</th>\n",
       "      <th>HeartRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1014.000000</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>1014.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.871795</td>\n",
       "      <td>113.198225</td>\n",
       "      <td>76.460552</td>\n",
       "      <td>8.725986</td>\n",
       "      <td>98.665089</td>\n",
       "      <td>74.301775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.474386</td>\n",
       "      <td>18.403913</td>\n",
       "      <td>13.885796</td>\n",
       "      <td>3.293532</td>\n",
       "      <td>1.371384</td>\n",
       "      <td>8.088702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age   SystolicBP  DiastolicBP           BS     BodyTemp  \\\n",
       "count  1014.000000  1014.000000  1014.000000  1014.000000  1014.000000   \n",
       "mean     29.871795   113.198225    76.460552     8.725986    98.665089   \n",
       "std      13.474386    18.403913    13.885796     3.293532     1.371384   \n",
       "min      10.000000    70.000000    49.000000     6.000000    98.000000   \n",
       "25%      19.000000   100.000000    65.000000     6.900000    98.000000   \n",
       "50%      26.000000   120.000000    80.000000     7.500000    98.000000   \n",
       "75%      39.000000   120.000000    90.000000     8.000000    98.000000   \n",
       "max      70.000000   160.000000   100.000000    19.000000   103.000000   \n",
       "\n",
       "         HeartRate  \n",
       "count  1014.000000  \n",
       "mean     74.301775  \n",
       "std       8.088702  \n",
       "min       7.000000  \n",
       "25%      70.000000  \n",
       "50%      76.000000  \n",
       "75%      80.000000  \n",
       "max      90.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SystolicBP</th>\n",
       "      <th>DiastolicBP</th>\n",
       "      <th>BS</th>\n",
       "      <th>BodyTemp</th>\n",
       "      <th>HeartRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.416045</td>\n",
       "      <td>0.398026</td>\n",
       "      <td>0.473284</td>\n",
       "      <td>-0.255323</td>\n",
       "      <td>0.079798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SystolicBP</th>\n",
       "      <td>0.416045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.787006</td>\n",
       "      <td>0.425172</td>\n",
       "      <td>-0.286616</td>\n",
       "      <td>-0.023108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiastolicBP</th>\n",
       "      <td>0.398026</td>\n",
       "      <td>0.787006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.423824</td>\n",
       "      <td>-0.257538</td>\n",
       "      <td>-0.046151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BS</th>\n",
       "      <td>0.473284</td>\n",
       "      <td>0.425172</td>\n",
       "      <td>0.423824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.103493</td>\n",
       "      <td>0.142867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BodyTemp</th>\n",
       "      <td>-0.255323</td>\n",
       "      <td>-0.286616</td>\n",
       "      <td>-0.257538</td>\n",
       "      <td>-0.103493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HeartRate</th>\n",
       "      <td>0.079798</td>\n",
       "      <td>-0.023108</td>\n",
       "      <td>-0.046151</td>\n",
       "      <td>0.142867</td>\n",
       "      <td>0.098771</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Age  SystolicBP  DiastolicBP        BS  BodyTemp  HeartRate\n",
       "Age          1.000000    0.416045     0.398026  0.473284 -0.255323   0.079798\n",
       "SystolicBP   0.416045    1.000000     0.787006  0.425172 -0.286616  -0.023108\n",
       "DiastolicBP  0.398026    0.787006     1.000000  0.423824 -0.257538  -0.046151\n",
       "BS           0.473284    0.425172     0.423824  1.000000 -0.103493   0.142867\n",
       "BodyTemp    -0.255323   -0.286616    -0.257538 -0.103493  1.000000   0.098771\n",
       "HeartRate    0.079798   -0.023108    -0.046151  0.142867  0.098771   1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features and Target\n",
    "X = df.drop(\"SystolicBP\", axis = 1)\n",
    "y = df[\"SystolicBP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding as we have got one feature being categprical or binary  \n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data \n",
    "x_train, x_test, y_train, y_test = train_test_split (X, y, random_state = 0, test_size = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(811, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "x_train_sc = scaler.fit_transform(x_train)\n",
    "x_test_sc = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing PCA on the cancer data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (811, 7)\n",
      "PCA: (811, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA (n_components =2, random_state = 0 ) # specify how many component u want; less, it will not reduce it but use entire feature and find the principal component out of them \n",
    "\n",
    "# transform the data\n",
    "x_train_pca = pca.fit_transform(x_train_sc)\n",
    "x_test_pca = pca.transform(x_test_sc)\n",
    "\n",
    "\n",
    "# check the shape of both the original data and the transformed one\n",
    "\n",
    "print(f\"Original: {x_train_sc.shape}\")\n",
    "print(f\"PCA: {x_train_pca.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52394719,  0.24829747, -0.51976254, ..., -0.5322086 ,\n",
       "        -0.83041154, -0.68498387],\n",
       "       [-0.52394719,  0.61151761, -0.55351395, ..., -1.01977841,\n",
       "        -0.83041154,  1.4598884 ],\n",
       "       [-0.96649658, -0.11492267,  1.01132388, ...,  0.19914612,\n",
       "        -0.83041154, -0.68498387],\n",
       "       ...,\n",
       "       [-0.45018896,  0.24829747, -0.36941538, ..., -1.01977841,\n",
       "         1.20422218, -0.68498387],\n",
       "       [ 0.36115159, -0.47814281, -0.36941538, ..., -1.01977841,\n",
       "         1.20422218, -0.68498387],\n",
       "       [ 0.13987689,  1.70117803, -0.55351395, ...,  0.44293102,\n",
       "        -0.83041154, -0.68498387]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08292973, -0.17846865],\n",
       "       [-0.06243439,  1.03879285],\n",
       "       [-0.01710349,  0.87986315],\n",
       "       ...,\n",
       "       [-0.69808459, -1.54611042],\n",
       "       [-0.64858213, -1.54211686],\n",
       "       [ 1.09263981, -0.266307  ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pca # the two important features from already scaled features is seen below  using the principal component "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can see here, the 7features has been compressed to 2 based on feature importance which is gotten \n",
    "when each features are being compared with the 2 principal component and then the one with a high variance or low similarity stand to rep the features for analysis\n",
    "\n",
    "The question, where are the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51547591,  0.50757655,  0.55209063, -0.21219252,  0.11749082,\n",
       "        -0.33366976, -0.04100378],\n",
       "       [-0.11319525, -0.131922  ,  0.00481857,  0.38351501,  0.18369978,\n",
       "        -0.62441247,  0.63169605]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_ # This is the actual principal components or factors used to bench  the feature column selected amongst the scaled or normalized feature arrays or in summary this is the factors used to transform the x_train_sc summarization    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - - Advantage of the PCA is that it helps summarize the high dime=nsional data into a smaller one to reduce high computational cost having known what the summarizxatiuon level would have been \n",
    "    \n",
    "\n",
    " -- Imagining trying to do computation on about 30 features; this is not cost effective on computation; having benched with the principal component to know summarization level, if 10features against the initla (30features) can summarize to about 70% and then the main information still conserved; we can say  10nos features will get the model done and with short compoutational time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing correlation between principal components and original data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DiastolicBP</th>\n",
       "      <th>BS</th>\n",
       "      <th>BodyTemp</th>\n",
       "      <th>HeartRate</th>\n",
       "      <th>RiskLevel_low risk</th>\n",
       "      <th>RiskLevel_mid risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>First_comp</th>\n",
       "      <td>0.751544</td>\n",
       "      <td>0.740027</td>\n",
       "      <td>0.804926</td>\n",
       "      <td>-0.309368</td>\n",
       "      <td>0.171297</td>\n",
       "      <td>-0.486477</td>\n",
       "      <td>-0.059782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Second_comp</th>\n",
       "      <td>-0.145723</td>\n",
       "      <td>-0.169831</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>0.493720</td>\n",
       "      <td>0.236487</td>\n",
       "      <td>-0.803841</td>\n",
       "      <td>0.813217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Age  DiastolicBP        BS  BodyTemp  HeartRate  \\\n",
       "First_comp   0.751544     0.740027  0.804926 -0.309368   0.171297   \n",
       "Second_comp -0.145723    -0.169831  0.006203  0.493720   0.236487   \n",
       "\n",
       "             RiskLevel_low risk  RiskLevel_mid risk  \n",
       "First_comp            -0.486477           -0.059782  \n",
       "Second_comp           -0.803841            0.813217  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get to find a better way to write the code  \n",
    "pd.DataFrame(data = [[np.corrcoef(x_train[c], x_train_pca[:,n])[1,0]\n",
    "                      for n in range(pca.n_components_)] for c in x_train ], \n",
    "             \n",
    "             index = x_train.columns, \n",
    "             columns = [\"First_comp\", \"Second_comp\"] ).T \n",
    "# This is producing a correlation between the scaled and summarized version of the data and recall the scaled will be btewn 0 nand 1\n",
    "# this is then correlated with the initail value corresponing to each featuren  to produce a correlation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deduction \n",
    "\n",
    "As can see from the Dataframe, we can see that the dataframe will produce a frame with correlation comparison between  x_train(c) - which is the original individual feature values and the corresponding x_train_pca which is the transformed values from the PCA. It pops the first component first which is a show of how the frist component is much important based on much value it can capture; the second PC value is also but could only explain the unxplainable part of the first component.  \n",
    "\n",
    "'As can see above, we can see how each original or initial features (x_train or x_train_sc ) has an importance value to the each Principal component first and second \n",
    "\n",
    "\n",
    "IMPORTANCE OF THIS ANALYSIS\n",
    "\n",
    "- It brings an alternative to the corrlation approaoch we do use say df.corr which we then do the feature selection based on correlation level; same thing being done here too but in a diferent approach or way \n",
    "\n",
    "\n",
    "So based on the values shown above, Age, DiastolicBp, BS, Body temp and Risk-leve_low risk seems to make contribution on the systolic level whioch is our target. WIth positive corrlation is and increased rate to systolic but negavtive is an inverse rate or rather the increased Age, disstolic and body temp have a 50% risk of post partum in women  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HInt\n",
    "\n",
    "\n",
    "- Get the scaled trained values \n",
    "\n",
    "- Compress the data by gettng us 2 most important features or Data column  in an array format as u know it will return just as the normalized or its scaled version  \n",
    "\n",
    "- Then, get the principal component that had resulted to that 2-numpy array x_train_pca  \n",
    "\n",
    "- You can now get the principal component itself resulting to the set of important two data coloumn in the array format  \n",
    "\n",
    "- Recall, withe the usual correlation df.corr u can used that as a feature importance approach, the principal components \n",
    "\n",
    "Now use the numpy array to determine or get the feature importance among many feature based on the correlation with the target and also yank off feature based on high correlation btw themselves... As an alternative is using the x_train_pca which is the two features summarized based on the principal components and then using the two to find a correlation with individual features in the original data; getting the sets of correlative values can give better insights on feature importance levele of the feature which then a selection of improtance features to be used for the modelling  \n",
    "\n",
    "ANSWER \n",
    "\n",
    "The Pca showing the corrleation between features is just buttreesing the correlation earlier done using df.corr which can be used with that for feature selection  or elimination for model building  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXplained Variance Ratio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This refers to the percenatge of the variation in the data explained by the principal components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30366419, 0.23675491])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5404190928349023"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This infers that using 2components will be able to explain 54% of information in the original data\n",
    "\n",
    "but from above, 1component will explain only 30% and only the second will explain 24%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if the two we used can oly explain 54%, now lets capture which of the n_component can explain much and better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_var = []\n",
    "for i in range(1,5): # Recall we have just 7 componnets so lets just summarize to just 4 features as it has to be lesser that the features nos  \n",
    "    pca = PCA(n_components = i, random_state = 0)\n",
    "    pca.fit_transform(x_train_sc)\n",
    "    exp =pca.explained_variance_ratio_.sum()\n",
    "    exp_var.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3036641851494061,\n",
       " 0.5404190928349023,\n",
       " 0.6966619311641322,\n",
       " 0.8169585751478899]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_var # using 1 feature will expalin 30% of the data; 2 will explain 54%; 3 will explain 69.6% etc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now having considered the no of sum components that will explain or summarize the data well, we will then transfrom via the PCA which then \n",
    "# to produce the x_train_pca and x_test_pca and then be passed into the model; say n_components that can summarize is 5, so n_component will be 5 \n",
    "\n",
    "# pca = PCA (n_components =2, random_state = 0 ) # specify how many component u want; less, it will not reduce it but use entire feature and find the principal component out of them \n",
    "\n",
    "# transform the data\n",
    "# x_train_pca = pca.fit_transform(x_train_sc)\n",
    "# x_test_pca = pca.transform(x_test_sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
